{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "# FINANCEPY BETA Version 0.290 - This build:  10 May 2023 at 18:52 #\n",
      "#     This software is distributed FREE AND WITHOUT ANY WARRANTY   #\n",
      "#  Report bugs as issues at https://github.com/domokane/FinancePy  #\n",
      "####################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\financepy\\products\\equity\\equity_binomial_tree.py:36: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @jit\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from Securities import Security\n",
    "# Add the parent directory (my_project) to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('boltmarket-main'), '..')))\n",
    "from importlib import reload\n",
    "import Tradables\n",
    "reload(Tradables)\n",
    "from Tradables import Underlier, MarketTable\n",
    "from Securities import Security, get_security\n",
    "from gs_quant.session import GsSession\n",
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "date_offsets = {\n",
    "    '1D': timedelta(days=1),\n",
    "    '1W': timedelta(weeks=1),\n",
    "    '2W': timedelta(weeks=2),\n",
    "    '3W': timedelta(weeks=3),\n",
    "    '1M': timedelta(days=30),\n",
    "    '2M': timedelta(days=60),\n",
    "    '3M': timedelta(days=90),\n",
    "    '6M': timedelta(days=180),\n",
    "    '9M': timedelta(days=270),\n",
    "    '1Y': timedelta(days=365),\n",
    "    'Overnight': timedelta(days=1),\n",
    "    'Tomorrow Next': timedelta(days=1),\n",
    "    'Spot Next': timedelta(days=2),\n",
    "    'One Week': timedelta(weeks=1),\n",
    "    'Two Weeks': timedelta(weeks=2),\n",
    "    'Three Weeks': timedelta(weeks=3),\n",
    "    'One Month': timedelta(days=30),\n",
    "    'Two Months': timedelta(days=60),\n",
    "    'Three Months': timedelta(days=90),\n",
    "    'Four Months': timedelta(days=120),\n",
    "    'Five Months': timedelta(days=150),\n",
    "    'Six Months': timedelta(days=180),\n",
    "    'Seven Months': timedelta(days=210),\n",
    "    'Eight Months': timedelta(days=240),\n",
    "    'Nine Months': timedelta(days=270),\n",
    "    'Ten Months': timedelta(days=300),\n",
    "    'Eleven Months': timedelta(days=330),\n",
    "    'One Year': timedelta(days=365),\n",
    "    'Two Years': timedelta(days=730),\n",
    "    'Three Years': timedelta(days=1095),\n",
    "    'Four Years': timedelta(days=1460),\n",
    "    'Five Years': timedelta(days=1825),\n",
    "    'Six Years': timedelta(days=2190),\n",
    "    'Seven Years': timedelta(days=2555),\n",
    "    'Ten Years': timedelta(days=3650),\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"use excel wings to load surface from EURUSD Risk&Marks\"\"\"\n",
    "guipath = os.path.abspath('C:\\\\Users\\\\jacob\\\\bolt-hub\\\\GUI.xlsx')\n",
    "marks = pd.read_excel(guipath, sheet_name=\"EURUSD Risks&Marks\")\n",
    "\n",
    "\n",
    "\n",
    "def save_security(sec):\n",
    "    \"\"\"save security to pickle in the securities folder\"\"\"\n",
    "    with open(f\"Securities/{sec.name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sec, f)\n",
    "\n",
    "def get_intraday_weighting():\n",
    "    ticker_symbol = 'EURUSD=X'\n",
    "    start_date = '2022-01-01'\n",
    "    end_date = '2023-06-01'\n",
    "    # Get the data from Yahoo Finance\n",
    "    data = yf.download(ticker_symbol, start=start_date, end=end_date, interval=\"1h\")\n",
    "\n",
    "    # Extract the open and close prices by hour\n",
    "    hourly_data = data[['Open', 'Close']].resample('H').last()\n",
    "\n",
    "    \"\"\"create a column representing hour of the day\"\"\"\n",
    "    hourly_data['Hour'] = hourly_data.index.hour\n",
    "    \"\"\"create a column representing change in price from previous hour\"\"\"\n",
    "    hourly_data['Change'] = abs(hourly_data['Open']-hourly_data['Close'])/hourly_data['Open']\n",
    "\n",
    "    \"\"\"group by hour and calculate mean change\"\"\"\n",
    "    hourly_data = hourly_data.groupby('Hour')['Change'].mean()\n",
    "    return hourly_data\n",
    "\n",
    "def extract_forward_curve(marks):\n",
    "    \"\"\"extract forward curve\"\"\"\n",
    "    fwd = marks.drop(index=range(5))\n",
    "    last_row_index = fwd.index[-31]\n",
    "    # Select all rows up to and including the last 31st row, and assign it back to the dataframe\n",
    "    fwd = fwd.iloc[:last_row_index+1, :]\n",
    "    fwd = fwd.iloc[:, :4]\n",
    "    fwd.iloc[1:, 0] = [date_offsets[tenor] for tenor in fwd.iloc[1:, 0]]\n",
    "    # get the values of the top row as a series\n",
    "    new_columns = fwd.iloc[0]\n",
    "    # assign the new column names to the dataframe\n",
    "    fwd.columns = new_columns\n",
    "    # drop the first row since it's no longer needed as column names\n",
    "    fwd = fwd.iloc[1:]\n",
    "    # set the index to the values in the first column by position\n",
    "    fwd = fwd.set_index(fwd.columns[0])\n",
    "    # drop the first column since it's now the index\n",
    "    fwd = fwd.drop(columns=[fwd.columns[0]])\n",
    "    fwd = fwd.rename_axis(index=None).rename_axis(columns=None)\n",
    "    fwd.drop('Bid', axis=1, inplace=True)\n",
    "    return fwd\n",
    "\n",
    "def extract_vol_surface(marks):\n",
    "    \"\"\"extract vol surface\"\"\"\n",
    "    vols = marks.drop(index=range(33))\n",
    "    vols = vols.iloc[0:, :18]\n",
    "    # get the values of the top row as a series\n",
    "    new_columns = [5,10,15,20,25,30,35,40,45,50,-40,-35,-30,-25,-20,-15,-10,-5]\n",
    "    # assign the new column names to the dataframe\n",
    "    vols.columns = new_columns\n",
    "    # drop the first row since it's no longer needed as column names\n",
    "    vols = vols.iloc[1:]\n",
    "    # set the index to the values in the first column by position\n",
    "    vols = vols.set_index(vols.columns[0])\n",
    "    # drop the first column since it's now the index\n",
    "    vols = vols.drop(columns=[vols.columns[0]])\n",
    "    vols = vols.rename_axis(index=None).rename_axis(columns=None)\n",
    "    new_index = [date_offsets[tenor] for tenor in vols.index]\n",
    "    vols.set_index(pd.Index(new_index), inplace=True)\n",
    "    return vols\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "eurusd = Underlier()\n",
    "eurusd.load_forward_curve(extract_forward_curve(marks))\n",
    "eurusd.load_vol_surface(extract_vol_surface(marks))\n",
    "eurusd.mark_spot(1.0935)\n",
    "eurusd.load_intraday_weights(get_intraday_weighting())\n",
    "\n",
    "sec = Security('EURUSD', eurusd)\n",
    "sec.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "usdjpy = Underlier()\n",
    "usdjpy.load_forward_curve(extract_forward_curve(marks))\n",
    "usdjpy.load_vol_surface(extract_vol_surface(marks))\n",
    "usdjpy.mark_spot(145)\n",
    "usdjpy.load_intraday_weights(get_intraday_weighting())\n",
    "\n",
    "sec = Security('USDJPY', usdjpy)\n",
    "sec.save()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "products = MarketTable(pd.DataFrame())\n",
    "Security('Tradables', products).save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sec = pickle.load(open('EURUSD.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sys.path.append('C:\\\\Users\\\\jacob\\\\bolt-hub\\\\boltmarket-main\\\\Securities')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     36\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://www.cmegroup.com/trading/fx/cme-fx-options-vol-converter.html\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Replace with the actual URL of the website you want to scrape\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m     \u001B[43mscrape_website\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[13], line 20\u001B[0m, in \u001B[0;36mscrape_website\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m     17\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Find the elements containing the articles' titles and links\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m article_titles \u001B[38;5;241m=\u001B[39m \u001B[43msoup\u001B[49m\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh2\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# You may need to adjust the tag based on the website's structure\u001B[39;00m\n\u001B[0;32m     21\u001B[0m article_links \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# You may need to adjust the tag based on the website's structure\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Extract and print the titles and links\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[13], line 20\u001B[0m, in \u001B[0;36mscrape_website\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m     17\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Find the elements containing the articles' titles and links\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m article_titles \u001B[38;5;241m=\u001B[39m \u001B[43msoup\u001B[49m\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh2\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# You may need to adjust the tag based on the website's structure\u001B[39;00m\n\u001B[0;32m     21\u001B[0m article_links \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# You may need to adjust the tag based on the website's structure\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Extract and print the titles and links\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.3\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_frame.py:880\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[1;34m(self, frame, event, arg)\u001B[0m\n\u001B[0;32m    877\u001B[0m             stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[1;32m--> 880\u001B[0m     stopped_on_plugin \u001B[38;5;241m=\u001B[39m \u001B[43mplugin_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_debugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[0;32m    882\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.3\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        # Add headers to the request to mimic a regular browser request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "\n",
    "        # Send an HTTP GET request to the URL with headers\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find the elements containing the articles' titles and links\n",
    "            article_titles = soup.find_all('h2')  # You may need to adjust the tag based on the website's structure\n",
    "            article_links = soup.find_all('a')  # You may need to adjust the tag based on the website's structure\n",
    "\n",
    "            # Extract and print the titles and links\n",
    "            for title, link in zip(article_titles, article_links):\n",
    "                print(\"Title:\", title.text.strip())\n",
    "                print(\"Link:\", link['href'])\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Error while sending the request:\", e)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.cmegroup.com/trading/fx/cme-fx-options-vol-converter.html\"  # Replace with the actual URL of the website you want to scrape\n",
    "    scrape_website(url)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_hourly_eurusd_data():\n",
    "    # Define the ticker symbol for EUR/USD\n",
    "    ticker = 'EURUSD=X'\n",
    "\n",
    "    # Define the start and end date for the data\n",
    "    start_date = '2021-09-01'\n",
    "    end_date = '2023-08-01'\n",
    "\n",
    "    # Fetch daily EUR/USD data from Yahoo Finance\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, progress=False, interval='1h')\n",
    "    return data\n",
    "\n",
    "data = get_hourly_eurusd_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "data['change'] = abs(data['Close'] - data['Open'])\n",
    "data['hour'] = data.index.hour\n",
    "by_hour = data.groupby('hour')['change'].mean()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "(by_hour.cumsum()/by_hour.cumsum().max()).to_csv('intraday_weighting.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
